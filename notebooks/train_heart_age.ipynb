{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, shutil, json\n",
    "from typing import List, Tuple\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import SimpleITK as sitk\n",
    "import scipy.spatial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import torch\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from monai import transforms\n",
    "from monai.utils import set_determinism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(object, path):\n",
    "    with open(path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(object, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n",
    "class HeartDataset(Dataset):\n",
    "    def __init__(self, data_dir, image_only=True, transforms=None, stats=None, permute=True):\n",
    "        super().__init__()\n",
    "        self.data = pd.read_csv(data_dir).dropna()\n",
    "        self.image_only = image_only\n",
    "        self.permute = permute\n",
    "\n",
    "        if self.image_only:\n",
    "            self.data = self.data[[\"study_id\", \"image\", \"mask\", \"age\"]]\n",
    "        else:\n",
    "            self.non_image_columns = [\n",
    "                col for col in self.data.columns \n",
    "                if col not in [\"study_id\", \"image\", \"mask\", \"age\"]\n",
    "            ]\n",
    "\n",
    "            # Compute stats if not provided\n",
    "            if stats is None:\n",
    "                self.height_mean = self.data[\"height\"].mean()\n",
    "                self.height_std = self.data[\"height\"].std()\n",
    "                self.weight_mean = self.data[\"weight\"].mean()\n",
    "                self.weight_std = self.data[\"weight\"].std()\n",
    "                self.stats = {\n",
    "                    \"height_mean\": self.height_mean,\n",
    "                    \"height_std\": self.height_std,\n",
    "                    \"weight_mean\": self.weight_mean,\n",
    "                    \"weight_std\": self.weight_std\n",
    "                }\n",
    "            else:\n",
    "                self.stats = stats\n",
    "                self.height_mean = stats[\"height_mean\"]\n",
    "                self.height_std = stats[\"height_std\"]\n",
    "                self.weight_mean = stats[\"weight_mean\"]\n",
    "                self.weight_std = stats[\"weight_std\"]\n",
    "\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_dir = self.data.iloc[idx][\"image\"]\n",
    "        age = torch.tensor([self.data.iloc[idx][\"age\"]], dtype=torch.float32)\n",
    "        non_image_data = torch.tensor([0])\n",
    "\n",
    "        if self.image_only:\n",
    "            img = self.transforms(img_dir) if self.transforms else img_dir\n",
    "        else:\n",
    "            img = self.transforms(img_dir) if self.transforms else img_dir\n",
    "            non_image_data = self.data.iloc[idx][self.non_image_columns]\n",
    "\n",
    "            # Standardize height and weight using training stats\n",
    "            non_image_data[\"height\"] = (non_image_data[\"height\"] - self.height_mean) / self.height_std\n",
    "            non_image_data[\"weight\"] = (non_image_data[\"weight\"] - self.weight_mean) / self.weight_std\n",
    "\n",
    "            # Convert non-image data to tensor\n",
    "            non_image_data = non_image_data.values.astype('float32')\n",
    "            non_image_data = torch.tensor(non_image_data, dtype=torch.float32)\n",
    "        if self.permute:\n",
    "            img = torch.permute(img, (0, 3, 2, 1)) # Channel, Axial, Coronal, Sagittal\n",
    "        return img, non_image_data, age\n",
    "\n",
    "\n",
    "class RegressionSFCNTorch(nn.Module):\n",
    "    def __init__(self, *, in_ch: int=1,\n",
    "        dropout: float=.0,\n",
    "        include_top: bool=True,\n",
    "        depths: List[int]=[32, 64, 128, 256, 256, 64],\n",
    "        prediction_range: Tuple[float, float]=(3.0, 100.0),\n",
    "        num_non_image: int=0,\n",
    "    ):\n",
    "        super(RegressionSFCNTorch, self).__init__()\n",
    "        self.include_top = include_top\n",
    "        self.prediction_range = prediction_range\n",
    "        self.num_non_image = num_non_image\n",
    "        self.block1 = nn.Sequential(OrderedDict([\n",
    "            ('block1_conv', nn.Conv3d(in_ch, depths[0], kernel_size=(3, 3, 3), stride=1, padding='same')),\n",
    "            ('block1_norm', nn.BatchNorm3d(num_features=depths[0], momentum=0.01, eps=0.001)),\n",
    "            ('block1_relu', nn.ReLU()),\n",
    "            ('block1_pool', nn.MaxPool3d(kernel_size=(2, 2, 2)))\n",
    "        ]))\n",
    "        self.block2 = nn.Sequential(OrderedDict([\n",
    "            ('block2_conv', nn.Conv3d(depths[0], depths[1], kernel_size=(3, 3, 3), stride=1, padding='same')),\n",
    "            ('block2_norm', nn.BatchNorm3d(num_features=depths[1], momentum=0.01, eps=0.001)),\n",
    "            ('block2_relu', nn.ReLU()),\n",
    "            ('block2_pool', nn.MaxPool3d(kernel_size=(2, 2, 2)))\n",
    "        ]))\n",
    "        self.block3 = nn.Sequential(OrderedDict([\n",
    "            ('block3_conv', nn.Conv3d(depths[1], depths[2], kernel_size=(3, 3, 3), stride=1, padding='same')),\n",
    "            ('block3_norm', nn.BatchNorm3d(num_features=depths[2], momentum=0.01, eps=0.001)),\n",
    "            ('block3_relu', nn.ReLU()),\n",
    "            ('block3_pool', nn.MaxPool3d(kernel_size=(2, 2, 2)))\n",
    "        ]))\n",
    "        self.block4 = nn.Sequential(OrderedDict([\n",
    "            ('block4_conv', nn.Conv3d(depths[2], depths[3], kernel_size=(3, 3, 3), stride=1, padding='same')),\n",
    "            ('block4_norm', nn.BatchNorm3d(num_features=depths[3], momentum=0.01, eps=0.001)),\n",
    "            ('block4_relu', nn.ReLU()),\n",
    "            ('block4_pool', nn.MaxPool3d(kernel_size=(1, 2, 2)))\n",
    "        ]))\n",
    "        self.block5 = nn.Sequential(OrderedDict([\n",
    "            ('block5_conv', nn.Conv3d(depths[3], depths[4], kernel_size=(3, 3, 3), stride=1, padding='same')),\n",
    "            ('block5_norm', nn.BatchNorm3d(num_features=depths[4], momentum=0.01, eps=0.001)),\n",
    "            ('block5_relu', nn.ReLU()),\n",
    "            ('block5_pool', nn.MaxPool3d(kernel_size=(1, 2, 2)))\n",
    "        ]))\n",
    "        self.top = nn.Sequential(OrderedDict([\n",
    "            ('top_conv', nn.Conv3d(depths[4], depths[5], kernel_size=(1, 1, 1), stride=1, padding='same')),\n",
    "            ('top_norm', nn.BatchNorm3d(num_features=depths[5], momentum=0.01, eps=0.001)),\n",
    "            ('top_relu', nn.ReLU()),\n",
    "            ('top_pool', nn.AvgPool3d(kernel_size=(6, 4, 4))),\n",
    "        ]))\n",
    "        self.dropout = nn.Sequential(OrderedDict([\n",
    "            ('top_dropout', nn.Dropout(p=dropout))\n",
    "        ]))\n",
    "        self.prediction = nn.Sequential(OrderedDict([\n",
    "            ('predictions', nn.Linear(in_features=depths[5] + self.num_non_image, out_features=1))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x, x_non_image=None):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        if self.include_top:\n",
    "            x = self.top(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.dropout(x)\n",
    "        if self.num_non_image > 0:\n",
    "            x = torch.cat((x, x_non_image), dim=1)\n",
    "        x = self.prediction(x)\n",
    "        x = x.reshape(-1,1)\n",
    "        return x\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def train_step(model, dataloader, loss_fn, optimizer, device):\n",
    "    # Put model in train mode\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    # Set up train loss\n",
    "    train_loss = 0\n",
    "    train_mae = 0\n",
    "    # Loop through data loader to get data batch\n",
    "    for batch, (X, N, y) in enumerate(dataloader):\n",
    "        # Send data to target device\n",
    "        X, y = X.float().to(device), y.float().to(device)\n",
    "        if N.any():\n",
    "            N = N.float().to(device)\n",
    "        # 1. Forward pass\n",
    "        if N.any():\n",
    "            y_pred = model(X, N)\n",
    "        else:\n",
    "            y_pred = model(X)\n",
    "        # 2. Calculate and accumulate loss\n",
    "        loss = loss_fn(y, y_pred)\n",
    "        train_loss += loss.item()\n",
    "        batch_mae = mean_absolute_error(y.detach().cpu().numpy(), y_pred.detach().cpu().numpy())\n",
    "        train_mae += batch_mae\n",
    "        # 3. Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "        # 4. Loss backward\n",
    "        loss.backward()\n",
    "        # 5. Optimizer step\n",
    "        optimizer.step()\n",
    "        print(f\"Batch number {(batch+1)}/{len(dataloader)}: Train MAE: {batch_mae} -- Train MSE: {loss.item()}\")\n",
    "    # Adjust metrics to get average loss and accuracy per batch\n",
    "    train_loss /= len(dataloader)\n",
    "    train_mae /= len(dataloader)\n",
    "    return train_loss, train_mae\n",
    "\n",
    "\n",
    "def test_step(model, dataloader, loss_fn, device):\n",
    "    # Put model in eval model\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    # Setup test loss\n",
    "    test_loss = 0\n",
    "    test_mae = 0\n",
    "    # Turn on inference context manager\n",
    "    with torch.no_grad():\n",
    "        for batch, (X, N, y) in enumerate(dataloader):\n",
    "            # Send data to target device\n",
    "            X, y = X.float().to(device), y.float().to(device)\n",
    "            if N.any():\n",
    "                N = N.float().to(device)\n",
    "            # 1. Forward\n",
    "            if N.any():\n",
    "                test_pred = model(X, N)\n",
    "            else:\n",
    "                test_pred = model(X)\n",
    "            # 2. Calculate and accumulate loss\n",
    "            loss = loss_fn(y, test_pred)\n",
    "            test_loss += loss.item()\n",
    "            test_mae += mean_absolute_error(y.cpu().numpy(), test_pred.cpu().numpy())\n",
    "        # Adjust metrics to get average loss per batch\n",
    "        test_loss = test_loss / len(dataloader)\n",
    "        test_mae = test_mae / len(dataloader)\n",
    "    return test_loss, test_mae\n",
    "\n",
    "\n",
    "class SaveBestModel:\n",
    "    \"\"\"\n",
    "    Class to save the best model while training. If the current epoch's\n",
    "    validation loss is less than the previous least less, then save the\n",
    "    model state.\n",
    "    \"\"\"\n",
    "    def __init__(self, best_valid_loss=float('inf')):\n",
    "        self.best_valid_loss = best_valid_loss\n",
    "\n",
    "    def __call__(self, save_dir, current_valid_loss, current_valid_mae, epoch, model, optimizer, loss_fn):\n",
    "        if current_valid_loss < self.best_valid_loss:\n",
    "            self.best_valid_loss = current_valid_loss\n",
    "            print(f\"\\nBest validation loss: {self.best_valid_loss:.5} | MAE: {current_valid_mae:.3f}\")\n",
    "            print(f\"\\nSaving best model for epoch: {epoch + 1}\\n\")\n",
    "            res = {'epoch': epoch + 1,\n",
    "                   'model_state_dict': model.state_dict(),\n",
    "                   'optimizer_state_dict': optimizer.state_dict(),\n",
    "                   'loss': loss_fn}\n",
    "            save_name = f'ep{epoch}_val-loss={current_valid_loss:.3f}_val-mae={current_valid_mae:.3f}.pth'\n",
    "            torch.save(res, os.path.join(save_dir, save_name))\n",
    "\n",
    "\n",
    "def save_model(save_dir, epoch, model, optimizer, loss_fn):\n",
    "    \"\"\"\n",
    "    Function to save the trained model to disk.\n",
    "    \"\"\"\n",
    "    save_name = f'ep{epoch}_final_model.pth'\n",
    "    assert save_name.endswith(\".pth\") or save_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n",
    "    print(f'[INFO] Saving model to: {save_name}')\n",
    "    res = {'model_state_dict': model.state_dict(),\n",
    "           'optimizer_state_dict': optimizer.state_dict(),\n",
    "           'loss': loss_fn}\n",
    "    torch.save(res, os.path.join(save_dir, save_name))\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device, save_dir):\n",
    "    save_best_model = SaveBestModel()\n",
    "\n",
    "    # Create a dictionary to save the training progress\n",
    "    results = {'train_loss': [], 'test_loss': [], \"train_mae\": [], \"test_mae\": []}\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "        train_loss, train_mae = train_step(model=model, dataloader=train_dataloader,\n",
    "                                loss_fn=loss_fn, optimizer=optimizer, device=device)\n",
    "        test_loss, test_mae = test_step(model=model, dataloader=test_dataloader,\n",
    "                              loss_fn=loss_fn, device=device)\n",
    "        # Save the mest model till now if we have the least loss in the current epoch\n",
    "        save_best_model(save_dir=save_dir, current_valid_loss=test_loss,\n",
    "                        current_valid_mae=test_mae, epoch=epoch,\n",
    "                        model=model, optimizer=optimizer, loss_fn=loss_fn)\n",
    "        # Print out what's happening\n",
    "        print(f\"Epoch: {epoch + 1} | train_loss: {train_loss:.5f} | test_loss: {test_loss:.5f} | \"\n",
    "              f\"train_mae: {train_mae:.3f} | test_mae: {test_mae:.3f}\")\n",
    "        # Update results dictionary\n",
    "        results['train_loss'].append(train_loss)\n",
    "        results['train_mae'].append(train_mae)\n",
    "        results['test_loss'].append(test_loss)\n",
    "        results['test_mae'].append(test_mae)\n",
    "\n",
    "        # Save train_loss and test_loss results\n",
    "        history_filename = os.path.join(save_dir, 'loss_results.csv')\n",
    "        pd.DataFrame(results).to_csv(history_filename, index=False)\n",
    "        print(f'\\nTrain loss and test loss history were saved in {history_filename}')\n",
    "\n",
    "\n",
    "class Config():\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_id,\n",
    "        result_id,\n",
    "        batch_size=2,\n",
    "        num_no_image=0,\n",
    "        image_only=True,\n",
    "        epochs=250,\n",
    "        lr=0.01,\n",
    "        seed=216,\n",
    "        dropout=0.0,\n",
    "        depths=[32, 64, 128, 256, 256, 64],\n",
    "    ):\n",
    "        self.data_id = data_id\n",
    "        self.result_id = result_id\n",
    "        self.batch_size = batch_size\n",
    "        self.num_no_image = num_no_image\n",
    "        self.image_only = image_only\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.seed = seed\n",
    "        self.dropout = dropout\n",
    "        self.depths = depths\n",
    "\n",
    "    def toJSON(self):\n",
    "        return json.dumps(\n",
    "            self,\n",
    "            default=lambda o: o.__dict__,\n",
    "            sort_keys=True,\n",
    "            indent=4\n",
    "        )\n",
    "\n",
    "    @classmethod\n",
    "    def fromJSON(cls, path):\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            json_str = json.load(f)\n",
    "        data = json.loads(json_str)\n",
    "        \n",
    "        return cls(\n",
    "            data_id=data['data_id'],\n",
    "            result_id=data['result_id'],\n",
    "            batch_size=data['batch_size'],\n",
    "            num_no_image=data['num_no_image'],\n",
    "            image_only=data['image_only'],\n",
    "            epochs=data['epochs'],\n",
    "            lr=data['lr'],\n",
    "            seed=data['seed'],\n",
    "            dropout=data['dropout'],\n",
    "            depths=data['depths']\n",
    "        )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config = Config(\n",
    "        data_id=\"run_4_v2\",\n",
    "        result_id=\"run_4_4b\",\n",
    "        batch_size=8,\n",
    "        num_no_image=7,\n",
    "        image_only=False,\n",
    "        epochs=250,\n",
    "        lr=0.0005,\n",
    "        seed=216,\n",
    "        dropout=0.5,\n",
    "        depths=[32, 64, 128, 256, 128, 64],\n",
    "    )\n",
    "\n",
    "    run_id = config.data_id\n",
    "\n",
    "    res_run_id = config.result_id\n",
    "    results_dir = f\"../results/{res_run_id}/\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "    num_non_image = config.num_no_image\n",
    "    image_only = config.image_only\n",
    "    epochs = config.epochs\n",
    "    bs = config.batch_size\n",
    "    lr = config.lr\n",
    "    seed = config.seed\n",
    "    depths = config.depths\n",
    "    dropout = config.dropout\n",
    "\n",
    "    config_json = config.toJSON()\n",
    "    save_json(config_json, os.path.join(results_dir, \"config.json\"))\n",
    "\n",
    "    set_determinism(seed)\n",
    "\n",
    "    img_transforms = transforms.Compose([\n",
    "        transforms.LoadImage(),\n",
    "        transforms.EnsureChannelFirst(),\n",
    "        transforms.EnsureType(),\n",
    "        transforms.ScaleIntensityRangePercentiles(lower=0, upper=99.9, b_min=0, b_max=1, clip=True),\n",
    "    ])\n",
    "\n",
    "    train_ds = HeartDataset(data_dir=f\"../data/{run_id}/metadata/train_data.csv\", image_only=image_only, transforms=img_transforms)\n",
    "    if num_non_image > 0:\n",
    "        stats = train_ds.stats\n",
    "    else:\n",
    "        stats = None\n",
    "    valid_ds = HeartDataset(data_dir=f\"../data/{run_id}/metadata/valid_data.csv\", image_only=image_only, transforms=img_transforms, stats=stats)\n",
    "\n",
    "    # train_ds.data = train_ds.data[:16]\n",
    "    # valid_ds.data = valid_ds.data[:16]\n",
    "\n",
    "    print(\"Train size:\", train_ds.__len__(), \"\\nValid size:\", valid_ds.__len__())\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True, drop_last=True)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size=bs, shuffle=False, drop_last=True)\n",
    "\n",
    "    # model\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = RegressionSFCNTorch(\n",
    "        depths=depths,\n",
    "        dropout=dropout,\n",
    "        num_non_image=num_non_image,\n",
    "    ).to(device)\n",
    "    print(f\"Number of model parameters: {count_parameters(model)}\")\n",
    "\n",
    "    # Train\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    train(model, train_dl, valid_dl, optimizer, loss_fn, epochs, device, results_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnunet_v1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
